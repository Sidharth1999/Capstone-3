{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1xZxvYC0i8pjhcRyHs2L4Bc6teac2FFAI",
      "authorship_tag": "ABX9TyOI2LGqfmlS7Pv9gzPTaaPi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sidharth1999/Capstone-3/blob/main/Image_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpE3fSO6-CSm"
      },
      "source": [
        "#Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import pathlib\n",
        "import imageio\n",
        "import functools\n",
        "import math"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pyzXj-FLufL"
      },
      "source": [
        "dataDir = \"/content/drive/My Drive/Springboard-Capstone-3/data\"\n",
        "projectDir = \"/content/drive/My Drive/Springboard-Capstone-3\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGOtMCT9RLKi"
      },
      "source": [
        "# **Download Dataset from Kaggle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "uhs4ipfhJMc1",
        "outputId": "8cee756b-a57f-4c88-9c50-665aa18aa724"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1b966fe2-690d-44fc-89fe-3fc7c049393e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1b966fe2-690d-44fc-89fe-3fc7c049393e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"sidharthr1999\",\"key\":\"b06c0eb687ec8653837b4badf109296d\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y8ELj7UKvDe"
      },
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMi4RYmPK1az",
        "outputId": "4f898ea1-7ebf-427f-f923-df34177ba55e"
      },
      "source": [
        "# Verify that kaggle commands work\n",
        "! kaggle datasets list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "ref                                                         title                                              size  lastUpdated          downloadCount  \n",
            "----------------------------------------------------------  ------------------------------------------------  -----  -------------------  -------------  \n",
            "gpreda/reddit-vaccine-myths                                 Reddit Vaccine Myths                              229KB  2021-06-01 11:18:46           6831  \n",
            "crowww/a-large-scale-fish-dataset                           A Large Scale Fish Dataset                          3GB  2021-04-28 17:03:01           4086  \n",
            "imsparsh/musicnet-dataset                                   MusicNet Dataset                                   22GB  2021-02-18 14:12:19           1364  \n",
            "dhruvildave/wikibooks-dataset                               Wikibooks Dataset                                   1GB  2021-02-18 10:08:27           2128  \n",
            "mathurinache/twitter-edge-nodes                             Twitter Edge Nodes                                342MB  2021-03-08 06:43:04            459  \n",
            "fatiimaezzahra/famous-iconic-women                          Famous Iconic Women                               838MB  2021-02-28 14:56:00            689  \n",
            "promptcloud/careerbuilder-job-listing-2020                  Careerbuilder Job Listing 2020                     42MB  2021-03-05 06:59:52            978  \n",
            "alsgroup/end-als                                            End ALS Kaggle Challenge                           12GB  2021-04-08 12:16:37            698  \n",
            "coloradokb/dandelionimages                                  DandelionImages                                     4GB  2021-02-19 20:03:47            412  \n",
            "nickuzmenkov/nih-chest-xrays-tfrecords                      NIH Chest X-rays TFRecords                         11GB  2021-03-09 04:49:23            554  \n",
            "simiotic/github-code-snippets                               GitHub Code Snippets                                7GB  2021-03-03 11:34:39            146  \n",
            "mathurinache/the-lj-speech-dataset                          The LJ Speech Dataset                               3GB  2021-02-15 09:19:54            170  \n",
            "stuartjames/lights                                          LightS: Light Specularity Dataset                  18GB  2021-02-18 14:32:26             66  \n",
            "landrykezebou/lvzhdr-tone-mapping-benchmark-dataset-tmonet  LVZ-HDR Tone Mapping Benchmark Dataset (TMO-Net)   24GB  2021-03-01 05:03:40             84  \n",
            "nickuzmenkov/ranzcr-clip-kfold-tfrecords                    RANZCR CLiP KFold TFRecords                         2GB  2021-02-21 13:29:51             82  \n",
            "imsparsh/accentdb-core-extended                             AccentDB - Core & Extended                          6GB  2021-02-17 14:22:54             77  \n",
            "datasnaek/youtube-new                                       Trending YouTube Video Statistics                 201MB  2019-06-03 00:56:47         141760  \n",
            "zynicide/wine-reviews                                       Wine Reviews                                       51MB  2017-11-27 17:08:04         137844  \n",
            "residentmario/ramen-ratings                                 Ramen Ratings                                      40KB  2018-01-11 16:04:39          23641  \n",
            "datasnaek/chess                                             Chess Game Dataset (Lichess)                        3MB  2017-09-04 03:09:09          18934  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w36rN7W2QMKf"
      },
      "source": [
        "dataset1 = \"iamsouravbanerjee/indian-food-images-dataset\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iCYLqDmLnpY",
        "outputId": "01c26230-5c86-4ca4-9a68-d4979a1e725e"
      },
      "source": [
        "#Download dataset\n",
        "!kaggle datasets download $dataset1 -p \"$dataDir\" --unzip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading indian-food-images-dataset.zip to /content/drive/My Drive/Springboard-Capstone-3/data\n",
            " 97% 343M/354M [00:02<00:00, 128MB/s]\n",
            "100% 354M/354M [00:03<00:00, 123MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZR7rAdQRSK0"
      },
      "source": [
        "# **Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6tfMKi4GsOr",
        "outputId": "da6a51be-fa5a-45d6-97f9-cf23ef010314"
      },
      "source": [
        "#Obtain all 80 categories\n",
        "categories = []\n",
        "for file in os.listdir(dataDir): categories.append(file)\n",
        "print(categories)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['adhirasam', 'aloo_gobi', 'aloo_matar', 'aloo_methi', 'aloo_shimla_mirch', 'aloo_tikki', 'anarsa', 'ariselu', 'bandar_laddu', 'basundi', 'bhatura', 'bhindi_masala', 'biryani', 'boondi', 'butter_chicken', 'chak_hao_kheer', 'cham_cham', 'chana_masala', 'chapati', 'chhena_kheeri', 'chicken_razala', 'chicken_tikka', 'chicken_tikka_masala', 'chikki', 'daal_baati_churma', 'daal_puri', 'dal_makhani', 'dal_tadka', 'dharwad_pedha', 'doodhpak', 'double_ka_meetha', 'dum_aloo', 'gajar_ka_halwa', 'gavvalu', 'ghevar', 'gulab_jamun', 'imarti', 'jalebi', 'kachori', 'kadai_paneer', 'kadhi_pakoda', 'kajjikaya', 'kakinada_khaja', 'kalakand', 'karela_bharta', 'kofta', 'kuzhi_paniyaram', 'lassi', 'ledikeni', 'litti_chokha', 'lyangcha', 'maach_jhol', 'makki_di_roti_sarson_da_saag', 'malapua', 'misi_roti', 'misti_doi', 'modak', 'mysore_pak', 'naan', 'navrattan_korma', 'palak_paneer', 'paneer_butter_masala', 'phirni', 'pithe', 'poha', 'poornalu', 'pootharekulu', 'qubani_ka_meetha', 'rabri', 'ras_malai', 'rasgulla', 'sandesh', 'shankarpali', 'sheer_korma', 'sheera', 'shrikhand', 'sohan_halwa', 'sohan_papdi', 'sutar_feni', 'unni_appam']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so_AyIDaIjCt",
        "outputId": "2052a1ec-adbe-460e-bec9-a7b432d18c05"
      },
      "source": [
        "#Extract all 4000 image paths for each of the 80 categories\n",
        "def imagePathsFromCategory(category):\n",
        "  generator = pathlib.Path(dataDir + '/' + category).glob('*.jpg')\n",
        "  sorted_paths = sorted([x for x in generator])\n",
        "  return sorted_paths \n",
        "\n",
        "image_paths = [imagePathsFromCategory(category) for category in categories]\n",
        "\n",
        "#Confirm all 4000 image paths were extracted:\n",
        "print(functools.reduce(lambda a, b: a+b, [len(categoryPaths) for categoryPaths in image_paths]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bjY_u6wOuKo"
      },
      "source": [
        "#Split each of the 80 categories into train, validation, and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "train = []\n",
        "validation = []\n",
        "test = []\n",
        "for i in range(len(image_paths)):\n",
        "  train_images, test_images, _, _ = train_test_split(image_paths[i], range(len(image_paths[i])), test_size=0.20, random_state=42)\n",
        "  train_images, validation_images, _, _ = train_test_split(train_images, range(len(train_images)), test_size=0.20, random_state=42)\n",
        "  train.append(train_images)\n",
        "  validation.append(validation_images)\n",
        "  test.append(test_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC1HsNuP79_u"
      },
      "source": [
        "#Make separate directories for each set\n",
        "os.mkdir(os.path.join(projectDir, \"train\"))\n",
        "os.mkdir(os.path.join(projectDir, \"validation\"))\n",
        "os.mkdir(os.path.join(projectDir, \"test\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5qytGwTbcK8"
      },
      "source": [
        "trainDir = projectDir + \"/\" + \"train\"\n",
        "validationDir = projectDir + \"/\" + \"validation\"\n",
        "testDir = projectDir + \"/\" + \"test\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdbvQkh2Vnrz"
      },
      "source": [
        "#Insert data from each category into the newly created train, validation, and test folders\n",
        "for i in range(len(categories)):\n",
        "  category = categories[i]\n",
        "  train_set = train[i]\n",
        "  validation_set = validation[i]\n",
        "  test_set = test[i]\n",
        "  trainDestDir = os.path.join(trainDir, category)\n",
        "  valDestDir = os.path.join(validationDir, category)\n",
        "  testDestDir = os.path.join(testDir, category)\n",
        "  os.mkdir(trainDestDir)\n",
        "  os.mkdir(valDestDir)\n",
        "  os.mkdir(testDestDir)\n",
        "\n",
        "  for j in range(len(train_set)):\n",
        "    impath = os.path.join(trainDestDir, f'image{j}.jpg')\n",
        "    imageio.imwrite(impath, imageio.imread(train_set[j]))\n",
        "\n",
        "  for j in range(len(validation_set)):\n",
        "    impath = os.path.join(valDestDir, f'image{j}.jpg')\n",
        "    imageio.imwrite(impath, imageio.imread(validation_set[j]))\n",
        "\n",
        "  for j in range(len(test_set)):\n",
        "    impath = os.path.join(testDestDir, f'image{j}.jpg')\n",
        "    imageio.imwrite(impath, imageio.imread(test_set[j]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYAdEqnYJnhs"
      },
      "source": [
        "# **Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yXbyJaXJuej"
      },
      "source": [
        "**Potential Configurable Parameters:**\n",
        "\n",
        "1.   Train/Dev/Test split-ratio\n",
        "2.   Number of layers\n",
        "3.   Number of nodes per layer\n",
        "4.   Optimizer\n",
        "5.   Learning rate of optimizer\n",
        "6.   Target image size\n",
        "7.   Generator morphological transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD4OtTc9Kcw0"
      },
      "source": [
        "**We are going to try 4 different CNN architectures:**\n",
        "\n",
        "\n",
        "1.   VGG16\n",
        "2.   DenseNet201\n",
        "3.   ResNet50\n",
        "4.   InceptionV3\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzUcQaNYr_L4"
      },
      "source": [
        "#Make a directory to save different CNN model features\n",
        "cnnFeaturesDir = os.path.join(projectDir, 'CNN Architecture Features')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTRK5zOcsC5U"
      },
      "source": [
        "os.mkdir(cnnFeaturesDir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5VUFZpmJSq4"
      },
      "source": [
        "## **VGG-16**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os05870cypgt",
        "outputId": "fc9c386d-1687-46ff-ac2c-7f1906ea7a51"
      },
      "source": [
        "#Build data generators\n",
        "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import vgg16\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        preprocessing_function = vgg16.preprocess_input,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(\n",
        "    preprocessing_function = vgg16.preprocess_input,\n",
        "    rescale=1./255)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        trainDir,\n",
        "        target_size=(40, 40),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validationDir,\n",
        "        target_size=(40, 40),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2560 images belonging to 80 classes.\n",
            "Found 640 images belonging to 80 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emhv1X-tMtV4"
      },
      "source": [
        "### **Transfer Learning Method #1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFcoSaHCy71-"
      },
      "source": [
        "#OHE for the labels\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "trainTarget = to_categorical(train_generator.labels)\n",
        "valTarget = to_categorical(validation_generator.labels)\n",
        "\n",
        "#Import model\n",
        "from tensorflow.keras.applications import VGG16\n",
        "vggModel = VGG16(include_top=False, weights='imagenet')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50Z1r0bQZ0_b"
      },
      "source": [
        "#Extract VGG16 Features - RUN ONLY ONCE AND THEN SAVE\n",
        "vggTrainFeatures = vggModel.predict(train_generator, batch_size=32, steps=int(math.ceil(train_generator.n/train_generator.batch_size)))\n",
        "vggValFeatures = vggModel.predict(validation_generator, batch_size=32, steps=int(math.ceil(validation_generator.n/validation_generator.batch_size)))\n",
        "np.save(cnnFeaturesDir + '/' + 'VGG16-train-features.npy', vggTrainFeatures)\n",
        "np.save(cnnFeaturesDir + '/' + 'VGG16-val-features.npy', vggValFeatures)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sphJa8_Emc1C",
        "outputId": "acc2749e-111a-4686-c1a7-afb9a539f3fa"
      },
      "source": [
        "#Retrieve features from project directory\n",
        "vggTrainData = np.load(cnnFeaturesDir + '/' + 'VGG16-train-features.npy')\n",
        "vggValData = np.load(cnnFeaturesDir + '/' + 'VGG16-val-features.npy')\n",
        "\n",
        "#Structure, compile and train the VGG16 model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization, LeakyReLU\n",
        "\n",
        "model1 = Sequential()\n",
        "model1.add(Flatten(input_shape=vggTrainData.shape[1:]))\n",
        "model1.add(Dense(100, activation=LeakyReLU(alpha=0.3)))\n",
        "model1.add(Dropout(0.5)) \n",
        "model1.add(Dense(50, activation=LeakyReLU(alpha=0.3))) \n",
        "model1.add(Dropout(0.3)) \n",
        "model1.add(Dense(80, activation='softmax'))\n",
        "\n",
        "model1.compile(optimizer='adam', metrics=['accuracy'], loss='categorical_crossentropy')\n",
        "\n",
        "history = model1.fit(vggTrainData, trainTarget, epochs=50, batch_size=32, validation_data=(vggValData, valTarget))\n",
        "\n",
        "(eval_loss, eval_accuracy) = model1.evaluate(vggValData, valTarget, batch_size=32, verbose=1)\n",
        "print(\"Accuracy: {:.2f}%\".format(eval_accuracy * 100)) \n",
        "print(\"Loss: {}\".format(eval_loss))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "80/80 [==============================] - 1s 6ms/step - loss: 4.4395 - accuracy: 0.0090 - val_loss: 4.3881 - val_accuracy: 0.0141\n",
            "Epoch 2/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 4.3855 - accuracy: 0.0152 - val_loss: 4.3934 - val_accuracy: 0.0156\n",
            "Epoch 3/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 4.3755 - accuracy: 0.0164 - val_loss: 4.3915 - val_accuracy: 0.0109\n",
            "Epoch 4/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 4.3581 - accuracy: 0.0211 - val_loss: 4.3920 - val_accuracy: 0.0156\n",
            "Epoch 5/50\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 4.3335 - accuracy: 0.0227 - val_loss: 4.4022 - val_accuracy: 0.0156\n",
            "Epoch 6/50\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 4.3153 - accuracy: 0.0340 - val_loss: 4.4125 - val_accuracy: 0.0125\n",
            "Epoch 7/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 4.2958 - accuracy: 0.0320 - val_loss: 4.4201 - val_accuracy: 0.0156\n",
            "Epoch 8/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 4.2689 - accuracy: 0.0355 - val_loss: 4.4290 - val_accuracy: 0.0078\n",
            "Epoch 9/50\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 4.2505 - accuracy: 0.0344 - val_loss: 4.4489 - val_accuracy: 0.0094\n",
            "Epoch 10/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 4.2092 - accuracy: 0.0508 - val_loss: 4.4515 - val_accuracy: 0.0094\n",
            "Epoch 11/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 4.1953 - accuracy: 0.0453 - val_loss: 4.4625 - val_accuracy: 0.0078\n",
            "Epoch 12/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 4.1583 - accuracy: 0.0527 - val_loss: 4.4876 - val_accuracy: 0.0125\n",
            "Epoch 13/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 4.1212 - accuracy: 0.0520 - val_loss: 4.4935 - val_accuracy: 0.0125\n",
            "Epoch 14/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 4.1043 - accuracy: 0.0629 - val_loss: 4.5061 - val_accuracy: 0.0109\n",
            "Epoch 15/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 4.0525 - accuracy: 0.0676 - val_loss: 4.5477 - val_accuracy: 0.0156\n",
            "Epoch 16/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 4.0402 - accuracy: 0.0695 - val_loss: 4.5512 - val_accuracy: 0.0156\n",
            "Epoch 17/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 4.0163 - accuracy: 0.0812 - val_loss: 4.5600 - val_accuracy: 0.0078\n",
            "Epoch 18/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 3.9862 - accuracy: 0.0754 - val_loss: 4.5744 - val_accuracy: 0.0063\n",
            "Epoch 19/50\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 3.9427 - accuracy: 0.0828 - val_loss: 4.5934 - val_accuracy: 0.0063\n",
            "Epoch 20/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 3.9201 - accuracy: 0.0848 - val_loss: 4.6096 - val_accuracy: 0.0063\n",
            "Epoch 21/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 3.8651 - accuracy: 0.0914 - val_loss: 4.6280 - val_accuracy: 0.0063\n",
            "Epoch 22/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 3.8517 - accuracy: 0.0973 - val_loss: 4.6235 - val_accuracy: 0.0094\n",
            "Epoch 23/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 3.8282 - accuracy: 0.0934 - val_loss: 4.6612 - val_accuracy: 0.0047\n",
            "Epoch 24/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 3.7840 - accuracy: 0.1063 - val_loss: 4.6875 - val_accuracy: 0.0078\n",
            "Epoch 25/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 3.7476 - accuracy: 0.1219 - val_loss: 4.7015 - val_accuracy: 0.0156\n",
            "Epoch 26/50\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 3.7600 - accuracy: 0.1191 - val_loss: 4.6747 - val_accuracy: 0.0109\n",
            "Epoch 27/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 3.7122 - accuracy: 0.1215 - val_loss: 4.7197 - val_accuracy: 0.0109\n",
            "Epoch 28/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 3.6832 - accuracy: 0.1238 - val_loss: 4.7404 - val_accuracy: 0.0109\n",
            "Epoch 29/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 3.6283 - accuracy: 0.1383 - val_loss: 4.7793 - val_accuracy: 0.0047\n",
            "Epoch 30/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 3.6066 - accuracy: 0.1453 - val_loss: 4.8178 - val_accuracy: 0.0094\n",
            "Epoch 31/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 3.6125 - accuracy: 0.1434 - val_loss: 4.8139 - val_accuracy: 0.0172\n",
            "Epoch 32/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 3.5831 - accuracy: 0.1461 - val_loss: 4.8189 - val_accuracy: 0.0172\n",
            "Epoch 33/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 3.5402 - accuracy: 0.1430 - val_loss: 4.8405 - val_accuracy: 0.0078\n",
            "Epoch 34/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 3.5283 - accuracy: 0.1551 - val_loss: 4.8736 - val_accuracy: 0.0125\n",
            "Epoch 35/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 3.4949 - accuracy: 0.1645 - val_loss: 4.8695 - val_accuracy: 0.0141\n",
            "Epoch 36/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 3.4655 - accuracy: 0.1641 - val_loss: 4.8896 - val_accuracy: 0.0063\n",
            "Epoch 37/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 3.4488 - accuracy: 0.1660 - val_loss: 4.9111 - val_accuracy: 0.0094\n",
            "Epoch 38/50\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 3.4313 - accuracy: 0.1715 - val_loss: 4.9195 - val_accuracy: 0.0109\n",
            "Epoch 39/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 3.4237 - accuracy: 0.1676 - val_loss: 4.9298 - val_accuracy: 0.0063\n",
            "Epoch 40/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 3.3603 - accuracy: 0.1750 - val_loss: 4.9661 - val_accuracy: 0.0063\n",
            "Epoch 41/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 3.3571 - accuracy: 0.1793 - val_loss: 5.0143 - val_accuracy: 0.0125\n",
            "Epoch 42/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 3.3030 - accuracy: 0.1918 - val_loss: 5.0657 - val_accuracy: 0.0109\n",
            "Epoch 43/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 3.3301 - accuracy: 0.1871 - val_loss: 5.0213 - val_accuracy: 0.0078\n",
            "Epoch 44/50\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 3.2904 - accuracy: 0.1945 - val_loss: 5.0394 - val_accuracy: 0.0141\n",
            "Epoch 45/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 3.2754 - accuracy: 0.2016 - val_loss: 5.0887 - val_accuracy: 0.0109\n",
            "Epoch 46/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 3.2503 - accuracy: 0.1945 - val_loss: 5.0868 - val_accuracy: 0.0125\n",
            "Epoch 47/50\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 3.2273 - accuracy: 0.2016 - val_loss: 5.1307 - val_accuracy: 0.0094\n",
            "Epoch 48/50\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 3.2333 - accuracy: 0.1984 - val_loss: 5.1045 - val_accuracy: 0.0125\n",
            "Epoch 49/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 3.1879 - accuracy: 0.2031 - val_loss: 5.1729 - val_accuracy: 0.0094\n",
            "Epoch 50/50\n",
            "80/80 [==============================] - 0s 4ms/step - loss: 3.2012 - accuracy: 0.2074 - val_loss: 5.1767 - val_accuracy: 0.0109\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 5.1767 - accuracy: 0.0109\n",
            "Accuracy: 1.09%\n",
            "Loss: 5.176727294921875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-DYQlVYM1xx"
      },
      "source": [
        "### **Transfer Learning Method #2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBw4h7jFbOg5",
        "outputId": "52daa8a6-4169-477b-de8f-e9c549b3e047"
      },
      "source": [
        "vggModel = tf.keras.applications.VGG16(input_shape=(40,40,3),include_top=False,weights='imagenet')\n",
        "vggModel.trainable = False\n",
        "inputs = tf.keras.Input(shape=(40, 40, 3))\n",
        "x = vggModel(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "outputs = tf.keras.layers.Dense(80, activation='softmax')(x)\n",
        "\n",
        "model1 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model1.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model1.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    validation_data=validation_generator,\n",
        "    steps_per_epoch=int(math.ceil(train_generator.n/train_generator.batch_size)),\n",
        "    validation_steps=int(math.ceil(validation_generator.n/validation_generator.batch_size)))\n",
        "\n",
        "(eval_loss, eval_accuracy) = model1.evaluate(validation_generator, verbose=1)\n",
        "print(\"Accuracy: {:.2f}%\".format(eval_accuracy * 100)) \n",
        "print(\"Loss: {}\".format(eval_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "80/80 [==============================] - 36s 444ms/step - loss: 4.3936 - accuracy: 0.0266 - val_loss: 4.2017 - val_accuracy: 0.0562\n",
            "Epoch 2/50\n",
            "80/80 [==============================] - 35s 437ms/step - loss: 4.0748 - accuracy: 0.0727 - val_loss: 4.0518 - val_accuracy: 0.0797\n",
            "Epoch 3/50\n",
            "80/80 [==============================] - 35s 437ms/step - loss: 3.8969 - accuracy: 0.1082 - val_loss: 3.9484 - val_accuracy: 0.1031\n",
            "Epoch 4/50\n",
            "80/80 [==============================] - 35s 440ms/step - loss: 3.7431 - accuracy: 0.1465 - val_loss: 3.8709 - val_accuracy: 0.1078\n",
            "Epoch 5/50\n",
            "80/80 [==============================] - 35s 438ms/step - loss: 3.6161 - accuracy: 0.1777 - val_loss: 3.8105 - val_accuracy: 0.1281\n",
            "Epoch 6/50\n",
            "80/80 [==============================] - 35s 442ms/step - loss: 3.5178 - accuracy: 0.2008 - val_loss: 3.7619 - val_accuracy: 0.1422\n",
            "Epoch 7/50\n",
            "80/80 [==============================] - 35s 442ms/step - loss: 3.4246 - accuracy: 0.2102 - val_loss: 3.7263 - val_accuracy: 0.1547\n",
            "Epoch 8/50\n",
            "80/80 [==============================] - 35s 444ms/step - loss: 3.3437 - accuracy: 0.2246 - val_loss: 3.6933 - val_accuracy: 0.1531\n",
            "Epoch 9/50\n",
            "80/80 [==============================] - 35s 442ms/step - loss: 3.2771 - accuracy: 0.2363 - val_loss: 3.6590 - val_accuracy: 0.1516\n",
            "Epoch 10/50\n",
            "80/80 [==============================] - 35s 440ms/step - loss: 3.2096 - accuracy: 0.2539 - val_loss: 3.6361 - val_accuracy: 0.1469\n",
            "Epoch 11/50\n",
            "80/80 [==============================] - 35s 437ms/step - loss: 3.1516 - accuracy: 0.2742 - val_loss: 3.6224 - val_accuracy: 0.1703\n",
            "Epoch 12/50\n",
            "80/80 [==============================] - 35s 439ms/step - loss: 3.0861 - accuracy: 0.2715 - val_loss: 3.6005 - val_accuracy: 0.1609\n",
            "Epoch 13/50\n",
            "80/80 [==============================] - 35s 436ms/step - loss: 3.0478 - accuracy: 0.2941 - val_loss: 3.5853 - val_accuracy: 0.1578\n",
            "Epoch 14/50\n",
            "80/80 [==============================] - 35s 434ms/step - loss: 2.9932 - accuracy: 0.3000 - val_loss: 3.5739 - val_accuracy: 0.1688\n",
            "Epoch 15/50\n",
            "80/80 [==============================] - 35s 436ms/step - loss: 2.9439 - accuracy: 0.3145 - val_loss: 3.5648 - val_accuracy: 0.1750\n",
            "Epoch 16/50\n",
            "80/80 [==============================] - 35s 436ms/step - loss: 2.8916 - accuracy: 0.3211 - val_loss: 3.5532 - val_accuracy: 0.1828\n",
            "Epoch 17/50\n",
            "80/80 [==============================] - 35s 439ms/step - loss: 2.8722 - accuracy: 0.3187 - val_loss: 3.5430 - val_accuracy: 0.1719\n",
            "Epoch 18/50\n",
            "80/80 [==============================] - 35s 435ms/step - loss: 2.8277 - accuracy: 0.3430 - val_loss: 3.5323 - val_accuracy: 0.1875\n",
            "Epoch 19/50\n",
            "80/80 [==============================] - 35s 438ms/step - loss: 2.7765 - accuracy: 0.3496 - val_loss: 3.5229 - val_accuracy: 0.1875\n",
            "Epoch 20/50\n",
            "80/80 [==============================] - 35s 437ms/step - loss: 2.7660 - accuracy: 0.3383 - val_loss: 3.5136 - val_accuracy: 0.1781\n",
            "Epoch 21/50\n",
            "80/80 [==============================] - 35s 437ms/step - loss: 2.7207 - accuracy: 0.3562 - val_loss: 3.5119 - val_accuracy: 0.1922\n",
            "Epoch 22/50\n",
            "80/80 [==============================] - 35s 432ms/step - loss: 2.6858 - accuracy: 0.3621 - val_loss: 3.5100 - val_accuracy: 0.1906\n",
            "Epoch 23/50\n",
            "80/80 [==============================] - 35s 435ms/step - loss: 2.6539 - accuracy: 0.3828 - val_loss: 3.5043 - val_accuracy: 0.1828\n",
            "Epoch 24/50\n",
            "80/80 [==============================] - 35s 438ms/step - loss: 2.6276 - accuracy: 0.3750 - val_loss: 3.5045 - val_accuracy: 0.2031\n",
            "Epoch 25/50\n",
            "80/80 [==============================] - 35s 436ms/step - loss: 2.5910 - accuracy: 0.3910 - val_loss: 3.4961 - val_accuracy: 0.1906\n",
            "Epoch 26/50\n",
            "80/80 [==============================] - 35s 437ms/step - loss: 2.5675 - accuracy: 0.3906 - val_loss: 3.4969 - val_accuracy: 0.1953\n",
            "Epoch 27/50\n",
            "30/80 [==========>...................] - ETA: 17s - loss: 2.4935 - accuracy: 0.4375"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQmWvaxgK3Ep"
      },
      "source": [
        "## **DenseNet201**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cekhb3Rf3dmW",
        "outputId": "5d7848c9-0fb5-402f-bde5-b1730a02a652"
      },
      "source": [
        "#Build data generators\n",
        "from tensorflow.keras.applications import densenet\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        preprocessing_function = densenet.preprocess_input,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(\n",
        "    preprocessing_function = densenet.preprocess_input,\n",
        "    rescale=1./255)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        trainDir,\n",
        "        target_size=(40, 40),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validationDir,\n",
        "        target_size=(40, 40),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2560 images belonging to 80 classes.\n",
            "Found 640 images belonging to 80 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK6CNxqZNVNu"
      },
      "source": [
        "### **Transfer Learning Method #1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_-W6EyB3eko"
      },
      "source": [
        "trainTarget = to_categorical(train_generator.labels)\n",
        "valTarget = to_categorical(validation_generator.labels)\n",
        "\n",
        "from tensorflow.keras.applications import DenseNet201\n",
        "DN201Model = DenseNet201(include_top=False, weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX901ZgQqHp5"
      },
      "source": [
        "DN201TrainFeatures = DN201Model.predict(train_generator)\n",
        "DN201ValFeatures = DN201Model.predict(validation_generator)\n",
        "np.save(cnnFeaturesDir + '/' + 'DN201-train-features.npy', DN201TrainFeatures)\n",
        "np.save(cnnFeaturesDir + '/' + 'DN201-val-features.npy', DN201ValFeatures)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abqK7bFkqgvM"
      },
      "source": [
        "DN201TrainData = np.load(cnnFeaturesDir + '/' + 'DN201-train-features.npy')\n",
        "DN201ValData = np.load(cnnFeaturesDir + '/' + 'DN201-val-features.npy')\n",
        "\n",
        "#Structure, compile and train the DenseNet201 model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization, LeakyReLU\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Flatten(input_shape=DN201TrainData.shape[1:]))\n",
        "model2.add(Dense(100, activation=LeakyReLU(alpha=0.3)))\n",
        "model2.add(Dropout(0.5)) \n",
        "model2.add(Dense(100, activation=LeakyReLU(alpha=0.3))) \n",
        "model2.add(Dropout(0.3)) \n",
        "model2.add(Dense(80, activation='softmax'))\n",
        "\n",
        "model2.compile(optimizer='adam', metrics=['accuracy'], loss='categorical_crossentropy')\n",
        "\n",
        "history = model2.fit(DN201TrainData, trainTarget, epochs=50, batch_size=32, validation_data=(DN201ValData, valTarget))\n",
        "\n",
        "(eval_loss, eval_accuracy) = model2.evaluate(DN201ValData, valTarget, batch_size=32, verbose=1)\n",
        "print(\"Accuracy: {:.2f}%\".format(eval_accuracy * 100)) \n",
        "print(\"Loss: {}\".format(eval_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ynKj-6rNZ1h"
      },
      "source": [
        "### **Transfer Learning Method #2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKKE3vRJ6qeO",
        "outputId": "d0131205-b59c-44b0-edf0-9dde2175ba48"
      },
      "source": [
        "DN201Model = tf.keras.applications.DenseNet201(input_shape=(40,40,3),include_top=False,weights='imagenet')\n",
        "DN201Model.trainable = False\n",
        "inputs = tf.keras.Input(shape=(40, 40, 3))\n",
        "x = DN201Model(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "outputs = tf.keras.layers.Dense(80, activation='softmax')(x)\n",
        "\n",
        "model2 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model2.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model2.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=validation_generator,\n",
        "    steps_per_epoch=train_generator.n//train_generator.batch_size,\n",
        "    validation_steps=validation_generator.n//validation_generator.batch_size)\n",
        "\n",
        "(eval_loss, eval_accuracy) = model2.evaluate(validation_generator, verbose=1)\n",
        "print(\"Accuracy: {:.2f}%\".format(eval_accuracy * 100)) \n",
        "print(\"Loss: {}\".format(eval_loss))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "74842112/74836368 [==============================] - 2s 0us/step\n",
            "Epoch 1/20\n",
            "80/80 [==============================] - 54s 509ms/step - loss: 4.4745 - accuracy: 0.0102 - val_loss: 4.4029 - val_accuracy: 0.0141\n",
            "Epoch 2/20\n",
            "80/80 [==============================] - 38s 470ms/step - loss: 4.4078 - accuracy: 0.0203 - val_loss: 4.3803 - val_accuracy: 0.0234\n",
            "Epoch 3/20\n",
            "80/80 [==============================] - 37s 467ms/step - loss: 4.3831 - accuracy: 0.0156 - val_loss: 4.3700 - val_accuracy: 0.0141\n",
            "Epoch 4/20\n",
            "80/80 [==============================] - 37s 466ms/step - loss: 4.3613 - accuracy: 0.0211 - val_loss: 4.3529 - val_accuracy: 0.0125\n",
            "Epoch 5/20\n",
            "80/80 [==============================] - 37s 468ms/step - loss: 4.3333 - accuracy: 0.0254 - val_loss: 4.3503 - val_accuracy: 0.0234\n",
            "Epoch 6/20\n",
            "80/80 [==============================] - 37s 468ms/step - loss: 4.3194 - accuracy: 0.0230 - val_loss: 4.3260 - val_accuracy: 0.0328\n",
            "Epoch 7/20\n",
            "80/80 [==============================] - 37s 468ms/step - loss: 4.2956 - accuracy: 0.0324 - val_loss: 4.3325 - val_accuracy: 0.0375\n",
            "Epoch 8/20\n",
            "80/80 [==============================] - 37s 464ms/step - loss: 4.2857 - accuracy: 0.0293 - val_loss: 4.3051 - val_accuracy: 0.0250\n",
            "Epoch 9/20\n",
            "80/80 [==============================] - 37s 464ms/step - loss: 4.2596 - accuracy: 0.0320 - val_loss: 4.3074 - val_accuracy: 0.0266\n",
            "Epoch 10/20\n",
            "80/80 [==============================] - 37s 469ms/step - loss: 4.2564 - accuracy: 0.0320 - val_loss: 4.2876 - val_accuracy: 0.0422\n",
            "Epoch 11/20\n",
            "80/80 [==============================] - 37s 467ms/step - loss: 4.2307 - accuracy: 0.0395 - val_loss: 4.2883 - val_accuracy: 0.0281\n",
            "Epoch 12/20\n",
            "80/80 [==============================] - 37s 465ms/step - loss: 4.2166 - accuracy: 0.0422 - val_loss: 4.2770 - val_accuracy: 0.0359\n",
            "Epoch 13/20\n",
            "80/80 [==============================] - 37s 464ms/step - loss: 4.2177 - accuracy: 0.0418 - val_loss: 4.2751 - val_accuracy: 0.0359\n",
            "Epoch 14/20\n",
            "80/80 [==============================] - 37s 466ms/step - loss: 4.1966 - accuracy: 0.0434 - val_loss: 4.2741 - val_accuracy: 0.0359\n",
            "Epoch 15/20\n",
            "80/80 [==============================] - 37s 464ms/step - loss: 4.1872 - accuracy: 0.0500 - val_loss: 4.2595 - val_accuracy: 0.0375\n",
            "Epoch 16/20\n",
            "80/80 [==============================] - 37s 463ms/step - loss: 4.1705 - accuracy: 0.0574 - val_loss: 4.2656 - val_accuracy: 0.0328\n",
            "Epoch 17/20\n",
            "80/80 [==============================] - 37s 465ms/step - loss: 4.1671 - accuracy: 0.0523 - val_loss: 4.2580 - val_accuracy: 0.0344\n",
            "Epoch 18/20\n",
            "80/80 [==============================] - 37s 466ms/step - loss: 4.1470 - accuracy: 0.0543 - val_loss: 4.2491 - val_accuracy: 0.0312\n",
            "Epoch 19/20\n",
            "80/80 [==============================] - 37s 464ms/step - loss: 4.1354 - accuracy: 0.0504 - val_loss: 4.2452 - val_accuracy: 0.0281\n",
            "Epoch 20/20\n",
            "80/80 [==============================] - 37s 465ms/step - loss: 4.1288 - accuracy: 0.0562 - val_loss: 4.2525 - val_accuracy: 0.0437\n",
            "20/20 [==============================] - 7s 333ms/step - loss: 4.2525 - accuracy: 0.0437\n",
            "Accuracy: 4.37%\n",
            "Loss: 4.252532005310059\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIeds2QyK79p"
      },
      "source": [
        "## **ResNet50**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_-oCPUn4ZQB"
      },
      "source": [
        "#Build data generators\n",
        "from tensorflow.keras.applications import resnet\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        preprocessing_function = resnet.preprocess_input,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(\n",
        "    preprocessing_function = resnet.preprocess_input,\n",
        "    rescale=1./255)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        trainDir,\n",
        "        target_size=(40, 40),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validationDir,\n",
        "        target_size=(40, 40),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJ_S_3bPNu13"
      },
      "source": [
        "### **Transfer Learning Method #1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUPumJfU4Z1S"
      },
      "source": [
        "trainTarget = to_categorical(train_generator.labels)\n",
        "valTarget = to_categorical(validation_generator.labels)\n",
        "\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "RN50Model = ResNet50(include_top=False, weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHCeU252s9SI"
      },
      "source": [
        "RN50TrainFeatures = RN50Model.predict(train_generator)\n",
        "RN50ValFeatures = RN50Model.predict(validation_generator)\n",
        "np.save(cnnFeaturesDir + '/' + 'RN50-train-features.npy', RN50TrainFeatures)\n",
        "np.save(cnnFeaturesDir + '/' + 'RN50-val-features.npy', RN50ValFeatures)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM0XMJmFtEeS"
      },
      "source": [
        "RN50TrainData = np.load(cnnFeaturesDir + '/' + 'RN50-train-features.npy')\n",
        "RN50ValData = np.load(cnnFeaturesDir + '/' + 'RN50-val-features.npy')\n",
        "\n",
        "#Structure, compile and train the ResNet50 model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization, LeakyReLU\n",
        "\n",
        "model3 = Sequential()\n",
        "model3.add(Flatten(input_shape=RN50TrainData.shape[1:]))\n",
        "model3.add(Dense(100, activation=LeakyReLU(alpha=0.3)))\n",
        "model3.add(Dropout(0.5)) \n",
        "model3.add(Dense(100, activation=LeakyReLU(alpha=0.3))) \n",
        "model3.add(Dropout(0.3)) \n",
        "model3.add(Dense(80, activation='softmax'))\n",
        "\n",
        "model3.compile(optimizer='adam', metrics=['accuracy'], loss='categorical_crossentropy')\n",
        "\n",
        "history = model3.fit(RN50TrainData, trainTarget, epochs=50, batch_size=32, validation_data=(RN50ValData, valTarget))\n",
        "\n",
        "(eval_loss, eval_accuracy) = model3.evaluate(RN50ValData, valTarget, batch_size=32, verbose=1)\n",
        "print(\"Accuracy: {:.2f}%\".format(eval_accuracy * 100)) \n",
        "print(\"Loss: {}\".format(eval_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTpzfmdxNzfV"
      },
      "source": [
        "### **Transfer Learning Method #2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A4y1AtN7LrF"
      },
      "source": [
        "RN50Model = tf.keras.applications.ResNet50(input_shape=(40,40,3),include_top=False,weights='imagenet')\n",
        "RN50Model.trainable = False\n",
        "inputs = tf.keras.Input(shape=(40, 40, 3))\n",
        "x = RN50Model(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "outputs = tf.keras.layers.Dense(80, activation='softmax')(x)\n",
        "\n",
        "model3 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model3.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model3.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=validation_generator,\n",
        "    steps_per_epoch=train_generator.n//train_generator.batch_size,\n",
        "    validation_steps=validation_generator.n//validation_generator.batch_size)\n",
        "\n",
        "(eval_loss, eval_accuracy) = model3.evaluate(validation_generator, verbose=1)\n",
        "print(\"Accuracy: {:.2f}%\".format(eval_accuracy * 100)) \n",
        "print(\"Loss: {}\".format(eval_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJP6lkQ5OKx9"
      },
      "source": [
        "## **InceptionV3**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do12nLJVOOJZ"
      },
      "source": [
        "#Build data generators\n",
        "from tensorflow.keras.applications import inception_v3\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        preprocessing_function = inception_v3.preprocess_input,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(\n",
        "    preprocessing_function = inception_v3.preprocess_input,\n",
        "    rescale=1./255)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        trainDir,\n",
        "        target_size=(40, 40),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validationDir,\n",
        "        target_size=(40, 40),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HscM2bpDOSBW"
      },
      "source": [
        "### **Transfer Learning Method #1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bElIWSw-OVRh"
      },
      "source": [
        "trainTarget = to_categorical(train_generator.labels)\n",
        "valTarget = to_categorical(validation_generator.labels)\n",
        "\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "IV3Model = InceptionV3(include_top=False, weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7GfyK77OYke"
      },
      "source": [
        "IV3TrainFeatures = IV3Model.predict(train_generator)\n",
        "IV3ValFeatures = IV3Model.predict(validation_generator)\n",
        "np.save(cnnFeaturesDir + '/' + 'IV3-train-features.npy', IV3TrainFeatures)\n",
        "np.save(cnnFeaturesDir + '/' + 'IV3-val-features.npy', IV3ValFeatures)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7SDB0toOa3b"
      },
      "source": [
        "IV3TrainData = np.load(cnnFeaturesDir + '/' + 'IV3-train-features.npy')\n",
        "IV3ValData = np.load(cnnFeaturesDir + '/' + 'IV3-val-features.npy')\n",
        "\n",
        "#Structure, compile and train the IV3 model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization, LeakyReLU\n",
        "\n",
        "model4 = Sequential()\n",
        "model4.add(Flatten(input_shape=IV3TrainData.shape[1:]))\n",
        "model4.add(Dense(100, activation=LeakyReLU(alpha=0.3)))\n",
        "model4.add(Dropout(0.5)) \n",
        "model4.add(Dense(100, activation=LeakyReLU(alpha=0.3))) \n",
        "model4.add(Dropout(0.3)) \n",
        "model4.add(Dense(80, activation='softmax'))\n",
        "\n",
        "model4.compile(optimizer='adam', metrics=['accuracy'], loss='categorical_crossentropy')\n",
        "\n",
        "history = model4.fit(IV3TrainData, trainTarget, epochs=50, batch_size=32, validation_data=(IV3ValData, valTarget))\n",
        "\n",
        "(eval_loss, eval_accuracy) = model4.evaluate(IV3ValData, valTarget, batch_size=32, verbose=1)\n",
        "print(\"Accuracy: {:.2f}%\".format(eval_accuracy * 100)) \n",
        "print(\"Loss: {}\".format(eval_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSWCI7TeOeZD"
      },
      "source": [
        "### **Transfer Learning Method #2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThseEBT2OhXH"
      },
      "source": [
        "IV3Model = tf.keras.applications.InceptionV3(input_shape=(40,40,3),include_top=False,weights='imagenet')\n",
        "IV3Model.trainable = False\n",
        "inputs = tf.keras.Input(shape=(40, 40, 3))\n",
        "x = IV3Model(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "outputs = tf.keras.layers.Dense(80, activation='softmax')(x)\n",
        "\n",
        "model4 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model4.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model4.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=validation_generator,\n",
        "    steps_per_epoch=train_generator.n//train_generator.batch_size,\n",
        "    validation_steps=validation_generator.n//validation_generator.batch_size)\n",
        "\n",
        "(eval_loss, eval_accuracy) = model4.evaluate(validation_generator, verbose=1)\n",
        "print(\"Accuracy: {:.2f}%\".format(eval_accuracy * 100)) \n",
        "print(\"Loss: {}\".format(eval_loss))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}