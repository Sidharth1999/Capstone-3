{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1xZxvYC0i8pjhcRyHs2L4Bc6teac2FFAI",
      "authorship_tag": "ABX9TyOnlMl3aDebHFIE/SIcqg9t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sidharth1999/Capstone-3/blob/main/Image_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpE3fSO6-CSm"
      },
      "source": [
        "#Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import pathlib\n",
        "import imageio\n",
        "import functools\n",
        "import math\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pyzXj-FLufL"
      },
      "source": [
        "dataDir = \"/content/drive/My Drive/Springboard-Capstone-3/data\"\n",
        "projectDir = \"/content/drive/My Drive/Springboard-Capstone-3\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGOtMCT9RLKi"
      },
      "source": [
        "# **Download Dataset from Kaggle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "uhs4ipfhJMc1",
        "outputId": "8cee756b-a57f-4c88-9c50-665aa18aa724"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1b966fe2-690d-44fc-89fe-3fc7c049393e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1b966fe2-690d-44fc-89fe-3fc7c049393e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"sidharthr1999\",\"key\":\"b06c0eb687ec8653837b4badf109296d\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y8ELj7UKvDe"
      },
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMi4RYmPK1az",
        "outputId": "4f898ea1-7ebf-427f-f923-df34177ba55e"
      },
      "source": [
        "# Verify that kaggle commands work\n",
        "! kaggle datasets list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "ref                                                         title                                              size  lastUpdated          downloadCount  \n",
            "----------------------------------------------------------  ------------------------------------------------  -----  -------------------  -------------  \n",
            "gpreda/reddit-vaccine-myths                                 Reddit Vaccine Myths                              229KB  2021-06-01 11:18:46           6831  \n",
            "crowww/a-large-scale-fish-dataset                           A Large Scale Fish Dataset                          3GB  2021-04-28 17:03:01           4086  \n",
            "imsparsh/musicnet-dataset                                   MusicNet Dataset                                   22GB  2021-02-18 14:12:19           1364  \n",
            "dhruvildave/wikibooks-dataset                               Wikibooks Dataset                                   1GB  2021-02-18 10:08:27           2128  \n",
            "mathurinache/twitter-edge-nodes                             Twitter Edge Nodes                                342MB  2021-03-08 06:43:04            459  \n",
            "fatiimaezzahra/famous-iconic-women                          Famous Iconic Women                               838MB  2021-02-28 14:56:00            689  \n",
            "promptcloud/careerbuilder-job-listing-2020                  Careerbuilder Job Listing 2020                     42MB  2021-03-05 06:59:52            978  \n",
            "alsgroup/end-als                                            End ALS Kaggle Challenge                           12GB  2021-04-08 12:16:37            698  \n",
            "coloradokb/dandelionimages                                  DandelionImages                                     4GB  2021-02-19 20:03:47            412  \n",
            "nickuzmenkov/nih-chest-xrays-tfrecords                      NIH Chest X-rays TFRecords                         11GB  2021-03-09 04:49:23            554  \n",
            "simiotic/github-code-snippets                               GitHub Code Snippets                                7GB  2021-03-03 11:34:39            146  \n",
            "mathurinache/the-lj-speech-dataset                          The LJ Speech Dataset                               3GB  2021-02-15 09:19:54            170  \n",
            "stuartjames/lights                                          LightS: Light Specularity Dataset                  18GB  2021-02-18 14:32:26             66  \n",
            "landrykezebou/lvzhdr-tone-mapping-benchmark-dataset-tmonet  LVZ-HDR Tone Mapping Benchmark Dataset (TMO-Net)   24GB  2021-03-01 05:03:40             84  \n",
            "nickuzmenkov/ranzcr-clip-kfold-tfrecords                    RANZCR CLiP KFold TFRecords                         2GB  2021-02-21 13:29:51             82  \n",
            "imsparsh/accentdb-core-extended                             AccentDB - Core & Extended                          6GB  2021-02-17 14:22:54             77  \n",
            "datasnaek/youtube-new                                       Trending YouTube Video Statistics                 201MB  2019-06-03 00:56:47         141760  \n",
            "zynicide/wine-reviews                                       Wine Reviews                                       51MB  2017-11-27 17:08:04         137844  \n",
            "residentmario/ramen-ratings                                 Ramen Ratings                                      40KB  2018-01-11 16:04:39          23641  \n",
            "datasnaek/chess                                             Chess Game Dataset (Lichess)                        3MB  2017-09-04 03:09:09          18934  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w36rN7W2QMKf"
      },
      "source": [
        "dataset1 = \"iamsouravbanerjee/indian-food-images-dataset\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iCYLqDmLnpY",
        "outputId": "01c26230-5c86-4ca4-9a68-d4979a1e725e"
      },
      "source": [
        "#Download dataset\n",
        "!kaggle datasets download $dataset1 -p \"$dataDir\" --unzip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading indian-food-images-dataset.zip to /content/drive/My Drive/Springboard-Capstone-3/data\n",
            " 97% 343M/354M [00:02<00:00, 128MB/s]\n",
            "100% 354M/354M [00:03<00:00, 123MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZR7rAdQRSK0"
      },
      "source": [
        "# **Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6tfMKi4GsOr",
        "outputId": "da6a51be-fa5a-45d6-97f9-cf23ef010314"
      },
      "source": [
        "#Obtain all 80 categories\n",
        "categories = []\n",
        "for file in os.listdir(dataDir): categories.append(file)\n",
        "print(categories)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['adhirasam', 'aloo_gobi', 'aloo_matar', 'aloo_methi', 'aloo_shimla_mirch', 'aloo_tikki', 'anarsa', 'ariselu', 'bandar_laddu', 'basundi', 'bhatura', 'bhindi_masala', 'biryani', 'boondi', 'butter_chicken', 'chak_hao_kheer', 'cham_cham', 'chana_masala', 'chapati', 'chhena_kheeri', 'chicken_razala', 'chicken_tikka', 'chicken_tikka_masala', 'chikki', 'daal_baati_churma', 'daal_puri', 'dal_makhani', 'dal_tadka', 'dharwad_pedha', 'doodhpak', 'double_ka_meetha', 'dum_aloo', 'gajar_ka_halwa', 'gavvalu', 'ghevar', 'gulab_jamun', 'imarti', 'jalebi', 'kachori', 'kadai_paneer', 'kadhi_pakoda', 'kajjikaya', 'kakinada_khaja', 'kalakand', 'karela_bharta', 'kofta', 'kuzhi_paniyaram', 'lassi', 'ledikeni', 'litti_chokha', 'lyangcha', 'maach_jhol', 'makki_di_roti_sarson_da_saag', 'malapua', 'misi_roti', 'misti_doi', 'modak', 'mysore_pak', 'naan', 'navrattan_korma', 'palak_paneer', 'paneer_butter_masala', 'phirni', 'pithe', 'poha', 'poornalu', 'pootharekulu', 'qubani_ka_meetha', 'rabri', 'ras_malai', 'rasgulla', 'sandesh', 'shankarpali', 'sheer_korma', 'sheera', 'shrikhand', 'sohan_halwa', 'sohan_papdi', 'sutar_feni', 'unni_appam']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so_AyIDaIjCt",
        "outputId": "2052a1ec-adbe-460e-bec9-a7b432d18c05"
      },
      "source": [
        "#Extract all 4000 image paths for each of the 80 categories\n",
        "def imagePathsFromCategory(category):\n",
        "  generator = pathlib.Path(dataDir + '/' + category).glob('*.jpg')\n",
        "  sorted_paths = sorted([x for x in generator])\n",
        "  return sorted_paths \n",
        "\n",
        "image_paths = [imagePathsFromCategory(category) for category in categories]\n",
        "\n",
        "#Confirm all 4000 image paths were extracted:\n",
        "print(functools.reduce(lambda a, b: a+b, [len(categoryPaths) for categoryPaths in image_paths]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bjY_u6wOuKo"
      },
      "source": [
        "#Split each of the 80 categories into train, validation, and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "train = []\n",
        "validation = []\n",
        "test = []\n",
        "for i in range(len(image_paths)):\n",
        "  train_images, test_images, _, _ = train_test_split(image_paths[i], range(len(image_paths[i])), test_size=0.20, random_state=42)\n",
        "  train_images, validation_images, _, _ = train_test_split(train_images, range(len(train_images)), test_size=0.20, random_state=42)\n",
        "  train.append(train_images)\n",
        "  validation.append(validation_images)\n",
        "  test.append(test_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC1HsNuP79_u"
      },
      "source": [
        "#Make separate directories for each set\n",
        "os.mkdir(os.path.join(projectDir, \"train\"))\n",
        "os.mkdir(os.path.join(projectDir, \"validation\"))\n",
        "os.mkdir(os.path.join(projectDir, \"test\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5qytGwTbcK8"
      },
      "source": [
        "trainDir = projectDir + \"/\" + \"train\"\n",
        "validationDir = projectDir + \"/\" + \"validation\"\n",
        "testDir = projectDir + \"/\" + \"test\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdbvQkh2Vnrz"
      },
      "source": [
        "#Insert data from each category into the newly created train, validation, and test folders\n",
        "for i in range(len(categories)):\n",
        "  category = categories[i]\n",
        "  train_set = train[i]\n",
        "  validation_set = validation[i]\n",
        "  test_set = test[i]\n",
        "  trainDestDir = os.path.join(trainDir, category)\n",
        "  valDestDir = os.path.join(validationDir, category)\n",
        "  testDestDir = os.path.join(testDir, category)\n",
        "  os.mkdir(trainDestDir)\n",
        "  os.mkdir(valDestDir)\n",
        "  os.mkdir(testDestDir)\n",
        "\n",
        "  for j in range(len(train_set)):\n",
        "    impath = os.path.join(trainDestDir, f'image{j}.jpg')\n",
        "    imageio.imwrite(impath, imageio.imread(train_set[j]))\n",
        "\n",
        "  for j in range(len(validation_set)):\n",
        "    impath = os.path.join(valDestDir, f'image{j}.jpg')\n",
        "    imageio.imwrite(impath, imageio.imread(validation_set[j]))\n",
        "\n",
        "  for j in range(len(test_set)):\n",
        "    impath = os.path.join(testDestDir, f'image{j}.jpg')\n",
        "    imageio.imwrite(impath, imageio.imread(test_set[j]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYAdEqnYJnhs"
      },
      "source": [
        "# **Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yXbyJaXJuej"
      },
      "source": [
        "**Potential Configurable Parameters:**\n",
        "\n",
        "1.   Train/Dev/Test split-ratio\n",
        "2.   Number of layers\n",
        "3.   Number of nodes per layer\n",
        "4.   Optimizer\n",
        "5.   Learning rate of optimizer\n",
        "6.   Target image size\n",
        "7.   Generator morphological transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD4OtTc9Kcw0"
      },
      "source": [
        "**We are going to try 4 different CNN architectures:**\n",
        "\n",
        "\n",
        "1.   VGG16\n",
        "2.   DenseNet201\n",
        "3.   ResNet50\n",
        "4.   InceptionV3\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzUcQaNYr_L4"
      },
      "source": [
        "#Make a directory to save different CNN model features\n",
        "cnnFeaturesDir = os.path.join(projectDir, 'CNN Architecture Features')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTRK5zOcsC5U"
      },
      "source": [
        "os.mkdir(cnnFeaturesDir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5VUFZpmJSq4"
      },
      "source": [
        "## **VGG-16**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os05870cypgt",
        "outputId": "1aa6622d-5d34-4a55-beea-8eecbb18880f"
      },
      "source": [
        "input_shape = (80, 80, 3)\n",
        "learning_rate = 0.001\n",
        "fine_tune = 1\n",
        "dropout = 0.2\n",
        "layer1 = 4096\n",
        "layer2 = 1072\n",
        "\n",
        "#Build data generators\n",
        "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import vgg16\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        preprocessing_function = vgg16.preprocess_input,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(\n",
        "    preprocessing_function = vgg16.preprocess_input,\n",
        "    rescale=1./255)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        trainDir,\n",
        "        target_size=input_shape[:2],\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validationDir,\n",
        "        target_size=input_shape[:2],\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        testDir,\n",
        "        target_size=input_shape[:2],\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2560 images belonging to 80 classes.\n",
            "Found 640 images belonging to 80 classes.\n",
            "Found 800 images belonging to 80 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-DYQlVYM1xx"
      },
      "source": [
        "### **Transfer Learning Method #2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBw4h7jFbOg5",
        "outputId": "75fd805e-7a0b-4ec6-fb07-ae695961873c"
      },
      "source": [
        "vggModel = tf.keras.applications.VGG16(input_shape=input_shape,include_top=False,weights='imagenet')\n",
        "if fine_tune > 0:\n",
        "  for layer in vggModel.layers[:-fine_tune]: layer.trainable = False\n",
        "else:\n",
        "  for layer in vggModel.layers: layer.trainable = False\n",
        "\n",
        "\n",
        "inputs = tf.keras.Input(shape=input_shape)\n",
        "x = vggModel(inputs, training=False)\n",
        "'''x = tf.keras.layers.GlobalAveragePooling2D()(x)'''\n",
        "x = tf.keras.layers.Flatten(name=\"flatten\")(x)\n",
        "x = tf.keras.layers.Dense(layer1, activation='relu')(x)\n",
        "x = tf.keras.layers.Dense(layer2, activation='relu')(x)\n",
        "x = tf.keras.layers.Dropout(dropout)(x)\n",
        "outputs = tf.keras.layers.Dense(80, activation='softmax')(x)\n",
        "\n",
        "model1 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model1.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    validation_data=validation_generator,\n",
        "    steps_per_epoch=int(math.ceil(train_generator.n/train_generator.batch_size)),\n",
        "    validation_steps=int(math.ceil(validation_generator.n/validation_generator.batch_size)))\n",
        "\n",
        "(eval_loss, eval_accuracy) = model1.evaluate(validation_generator, verbose=1)\n",
        "print(\"Accuracy: {:.2f}%\".format(eval_accuracy * 100)) \n",
        "print(\"Loss: {}\".format(eval_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "80/80 [==============================] - 32s 382ms/step - loss: 4.3396 - accuracy: 0.0336 - val_loss: 3.8591 - val_accuracy: 0.0859\n",
            "Epoch 2/50\n",
            "80/80 [==============================] - 30s 378ms/step - loss: 3.6255 - accuracy: 0.1086 - val_loss: 3.4321 - val_accuracy: 0.1547\n",
            "Epoch 3/50\n",
            "80/80 [==============================] - 30s 379ms/step - loss: 3.0967 - accuracy: 0.1961 - val_loss: 3.2588 - val_accuracy: 0.2047\n",
            "Epoch 4/50\n",
            "80/80 [==============================] - 30s 379ms/step - loss: 2.6910 - accuracy: 0.2883 - val_loss: 3.2435 - val_accuracy: 0.1969\n",
            "Epoch 5/50\n",
            "80/80 [==============================] - 30s 379ms/step - loss: 2.3230 - accuracy: 0.3633 - val_loss: 3.0601 - val_accuracy: 0.2313\n",
            "Epoch 6/50\n",
            "80/80 [==============================] - 30s 378ms/step - loss: 1.9902 - accuracy: 0.4355 - val_loss: 3.0111 - val_accuracy: 0.2406\n",
            "Epoch 7/50\n",
            "80/80 [==============================] - 30s 378ms/step - loss: 1.6641 - accuracy: 0.5312 - val_loss: 3.2029 - val_accuracy: 0.2562\n",
            "Epoch 8/50\n",
            "80/80 [==============================] - 30s 373ms/step - loss: 1.4153 - accuracy: 0.5949 - val_loss: 3.2552 - val_accuracy: 0.2656\n",
            "Epoch 9/50\n",
            "80/80 [==============================] - 30s 378ms/step - loss: 1.2329 - accuracy: 0.6477 - val_loss: 3.1861 - val_accuracy: 0.3000\n",
            "Epoch 10/50\n",
            "80/80 [==============================] - 30s 375ms/step - loss: 1.0057 - accuracy: 0.7059 - val_loss: 3.1927 - val_accuracy: 0.3141\n",
            "Epoch 11/50\n",
            "80/80 [==============================] - 30s 376ms/step - loss: 0.8250 - accuracy: 0.7664 - val_loss: 3.4506 - val_accuracy: 0.2875\n",
            "Epoch 12/50\n",
            "80/80 [==============================] - 30s 376ms/step - loss: 0.7296 - accuracy: 0.7906 - val_loss: 3.4793 - val_accuracy: 0.2969\n",
            "Epoch 13/50\n",
            "80/80 [==============================] - 30s 374ms/step - loss: 0.5933 - accuracy: 0.8246 - val_loss: 3.6815 - val_accuracy: 0.3078\n",
            "Epoch 14/50\n",
            "80/80 [==============================] - 30s 374ms/step - loss: 0.4967 - accuracy: 0.8539 - val_loss: 3.6511 - val_accuracy: 0.3234\n",
            "Epoch 15/50\n",
            "80/80 [==============================] - 30s 372ms/step - loss: 0.4472 - accuracy: 0.8719 - val_loss: 4.0024 - val_accuracy: 0.2984\n",
            "Epoch 16/50\n",
            "80/80 [==============================] - 30s 371ms/step - loss: 0.4058 - accuracy: 0.8832 - val_loss: 3.8487 - val_accuracy: 0.3031\n",
            "Epoch 17/50\n",
            "80/80 [==============================] - 30s 375ms/step - loss: 0.3843 - accuracy: 0.8801 - val_loss: 3.9512 - val_accuracy: 0.3094\n",
            "Epoch 18/50\n",
            "80/80 [==============================] - 30s 376ms/step - loss: 0.2928 - accuracy: 0.9156 - val_loss: 4.0148 - val_accuracy: 0.3094\n",
            "Epoch 19/50\n",
            "80/80 [==============================] - 30s 378ms/step - loss: 0.2757 - accuracy: 0.9187 - val_loss: 4.2353 - val_accuracy: 0.3156\n",
            "Epoch 20/50\n",
            " 1/80 [..............................] - ETA: 22s - loss: 0.3302 - accuracy: 0.8438"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEZtHh-yXvjP"
      },
      "source": [
        "### **Prediction Results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45q7Ll3FXy0q",
        "outputId": "8205e7ed-39dc-4583-af3f-ef17733aeb5f"
      },
      "source": [
        "results = model1.evaluate(test_generator, batch_size=32)\n",
        "print(\"test loss, test acc:\", results)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25/25 [==============================] - 6s 253ms/step - loss: 5.9049 - accuracy: 0.2512\n",
            "test loss, test acc: [5.9049482345581055, 0.2512499988079071]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rc3rUTMHoKc"
      },
      "source": [
        "model1.save(projectDir + '/' + 'Models/VGG16ModelV1.h5')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQmWvaxgK3Ep"
      },
      "source": [
        "## **DenseNet201**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cekhb3Rf3dmW",
        "outputId": "042a0329-d58e-4301-e7f3-d92822be9325"
      },
      "source": [
        "input_shape = (40, 40, 3)\n",
        "learning_rate = 0.001\n",
        "fine_tune = 1\n",
        "dropout = 0.2\n",
        "layer1 = 4096\n",
        "layer2 = 1072\n",
        "\n",
        "#Build data generators\n",
        "from tensorflow.keras.applications import densenet\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        preprocessing_function = densenet.preprocess_input,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(\n",
        "    preprocessing_function = densenet.preprocess_input,\n",
        "    rescale=1./255)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        trainDir,\n",
        "        target_size=input_shape[:2],\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validationDir,\n",
        "        target_size=input_shape[:2],\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        testDir,\n",
        "        target_size=input_shape[:2],\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2560 images belonging to 80 classes.\n",
            "Found 640 images belonging to 80 classes.\n",
            "Found 800 images belonging to 80 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ynKj-6rNZ1h"
      },
      "source": [
        "### **Transfer Learning Method #2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KKKE3vRJ6qeO",
        "outputId": "0d3d2e51-5d77-4847-96f3-0d22d123f76a"
      },
      "source": [
        "DN201Model = tf.keras.applications.DenseNet201(input_shape=input_shape,include_top=False,weights='imagenet')\n",
        "\n",
        "if fine_tune > 0:\n",
        "  for layer in DN201Model.layers[:-fine_tune]: layer.trainable = False\n",
        "else:\n",
        "  for layer in DN201Model.layers: layer.trainable = False\n",
        "\n",
        "inputs = tf.keras.Input(shape=input_shape)\n",
        "x = DN201Model(inputs, training=False)\n",
        "'''x = tf.keras.layers.GlobalAveragePooling2D()(x)'''\n",
        "x = tf.keras.layers.Flatten(name=\"flatten\")(x)\n",
        "x = tf.keras.layers.Dense(layer1, activation='relu')(x)\n",
        "x = tf.keras.layers.Dense(layer2, activation='relu')(x)\n",
        "x = tf.keras.layers.Dropout(dropout)(x)\n",
        "outputs = tf.keras.layers.Dense(80, activation='softmax')(x)\n",
        "\n",
        "model2 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model2.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    validation_data=validation_generator,\n",
        "    steps_per_epoch=int(math.ceil(train_generator.n/train_generator.batch_size)),\n",
        "    validation_steps=int(math.ceil(validation_generator.n/validation_generator.batch_size)))\n",
        "\n",
        "(eval_loss, eval_accuracy) = model2.evaluate(validation_generator, verbose=1)\n",
        "print(\"Accuracy: {:.2f}%\".format(eval_accuracy * 100)) \n",
        "print(\"Loss: {}\".format(eval_loss))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "74842112/74836368 [==============================] - 0s 0us/step\n",
            "Epoch 1/50\n",
            "80/80 [==============================] - 41s 391ms/step - loss: 4.4532 - accuracy: 0.0070 - val_loss: 4.3833 - val_accuracy: 0.0109\n",
            "Epoch 2/50\n",
            "80/80 [==============================] - 29s 361ms/step - loss: 4.3865 - accuracy: 0.0113 - val_loss: 4.3811 - val_accuracy: 0.0125\n",
            "Epoch 3/50\n",
            "80/80 [==============================] - 29s 361ms/step - loss: 4.3819 - accuracy: 0.0094 - val_loss: 4.3704 - val_accuracy: 0.0203\n",
            "Epoch 4/50\n",
            "80/80 [==============================] - 29s 362ms/step - loss: 4.3597 - accuracy: 0.0160 - val_loss: 4.3437 - val_accuracy: 0.0188\n",
            "Epoch 5/50\n",
            "80/80 [==============================] - 29s 358ms/step - loss: 4.3361 - accuracy: 0.0145 - val_loss: 4.3168 - val_accuracy: 0.0188\n",
            "Epoch 6/50\n",
            "80/80 [==============================] - 29s 363ms/step - loss: 4.2973 - accuracy: 0.0176 - val_loss: 4.3055 - val_accuracy: 0.0281\n",
            "Epoch 7/50\n",
            "80/80 [==============================] - 29s 357ms/step - loss: 4.2789 - accuracy: 0.0199 - val_loss: 4.2858 - val_accuracy: 0.0203\n",
            "Epoch 8/50\n",
            "80/80 [==============================] - 29s 358ms/step - loss: 4.2710 - accuracy: 0.0191 - val_loss: 4.2588 - val_accuracy: 0.0281\n",
            "Epoch 9/50\n",
            "80/80 [==============================] - 29s 359ms/step - loss: 4.2493 - accuracy: 0.0219 - val_loss: 4.2620 - val_accuracy: 0.0297\n",
            "Epoch 10/50\n",
            "80/80 [==============================] - 29s 361ms/step - loss: 4.2271 - accuracy: 0.0301 - val_loss: 4.2791 - val_accuracy: 0.0156\n",
            "Epoch 11/50\n",
            "80/80 [==============================] - 29s 363ms/step - loss: 4.2104 - accuracy: 0.0301 - val_loss: 4.2199 - val_accuracy: 0.0188\n",
            "Epoch 12/50\n",
            "80/80 [==============================] - 29s 359ms/step - loss: 4.1759 - accuracy: 0.0348 - val_loss: 4.2065 - val_accuracy: 0.0281\n",
            "Epoch 13/50\n",
            "80/80 [==============================] - 29s 362ms/step - loss: 4.1729 - accuracy: 0.0387 - val_loss: 4.2040 - val_accuracy: 0.0219\n",
            "Epoch 14/50\n",
            "80/80 [==============================] - 29s 363ms/step - loss: 4.1609 - accuracy: 0.0348 - val_loss: 4.2186 - val_accuracy: 0.0391\n",
            "Epoch 15/50\n",
            "80/80 [==============================] - 29s 362ms/step - loss: 4.1424 - accuracy: 0.0379 - val_loss: 4.1962 - val_accuracy: 0.0312\n",
            "Epoch 16/50\n",
            "80/80 [==============================] - 29s 357ms/step - loss: 4.1205 - accuracy: 0.0453 - val_loss: 4.2084 - val_accuracy: 0.0312\n",
            "Epoch 17/50\n",
            "80/80 [==============================] - 29s 361ms/step - loss: 4.0996 - accuracy: 0.0445 - val_loss: 4.1771 - val_accuracy: 0.0297\n",
            "Epoch 18/50\n",
            "80/80 [==============================] - 29s 359ms/step - loss: 4.1124 - accuracy: 0.0434 - val_loss: 4.2184 - val_accuracy: 0.0312\n",
            "Epoch 19/50\n",
            "80/80 [==============================] - 29s 359ms/step - loss: 4.0921 - accuracy: 0.0430 - val_loss: 4.1733 - val_accuracy: 0.0359\n",
            "Epoch 20/50\n",
            "11/80 [===>..........................] - ETA: 20s - loss: 4.0293 - accuracy: 0.0426"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-dd3931692d5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     validation_steps=int(math.ceil(validation_generator.n/validation_generator.batch_size)))\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0meval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_accuracy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hbNSB2tYC5L"
      },
      "source": [
        "### **Prediction Results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmfKKFsZYFEi"
      },
      "source": [
        "results = model2.evaluate(test_generator, batch_size=32)\n",
        "print(\"test loss, test acc:\", results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIeds2QyK79p"
      },
      "source": [
        "## **ResNet50**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_-oCPUn4ZQB",
        "outputId": "0b217d7c-85a4-4c8b-9ace-eb3a3b6e5ab8"
      },
      "source": [
        "input_shape = (40, 40, 3)\n",
        "learning_rate = 0.001\n",
        "fine_tune = 1\n",
        "dropout = 0.2\n",
        "layer1 = 4096\n",
        "layer2 = 1072\n",
        "\n",
        "#Build data generators\n",
        "from tensorflow.keras.applications import resnet\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        preprocessing_function = resnet.preprocess_input,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(\n",
        "    preprocessing_function = resnet.preprocess_input,\n",
        "    rescale=1./255)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        trainDir,\n",
        "        target_size=input_shape[:2],\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validationDir,\n",
        "        target_size=input_shape[:2],\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        testDir,\n",
        "        target_size=input_shape[:2],\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2560 images belonging to 80 classes.\n",
            "Found 640 images belonging to 80 classes.\n",
            "Found 800 images belonging to 80 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTpzfmdxNzfV"
      },
      "source": [
        "### **Transfer Learning Method #2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "4A4y1AtN7LrF",
        "outputId": "0b96e361-104b-455e-822e-c61662f41081"
      },
      "source": [
        "RN50Model = tf.keras.applications.ResNet50(input_shape=input_shape,include_top=False,weights='imagenet')\n",
        "\n",
        "if fine_tune > 0:\n",
        "  for layer in RN50Model.layers[:-fine_tune]: layer.trainable = False\n",
        "else:\n",
        "  for layer in RN50Model.layers: layer.trainable = False\n",
        "\n",
        "inputs = tf.keras.Input(shape=input_shape)\n",
        "x = RN50Model(inputs, training=False)\n",
        "'''x = tf.keras.layers.GlobalAveragePooling2D()(x)'''\n",
        "x = tf.keras.layers.Flatten(name=\"flatten\")(x)\n",
        "x = tf.keras.layers.Dense(layer1, activation='relu')(x)\n",
        "x = tf.keras.layers.Dense(layer2, activation='relu')(x)\n",
        "x = tf.keras.layers.Dropout(dropout)(x)\n",
        "outputs = tf.keras.layers.Dense(80, activation='softmax')(x)\n",
        "\n",
        "model3 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model3.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model3.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=validation_generator,\n",
        "    steps_per_epoch=int(math.ceil(train_generator.n/train_generator.batch_size)),\n",
        "    validation_steps=int(math.ceil(validation_generator.n/validation_generator.batch_size)))\n",
        "\n",
        "(eval_loss, eval_accuracy) = model3.evaluate(validation_generator, verbose=1)\n",
        "print(\"Accuracy: {:.2f}%\".format(eval_accuracy * 100)) \n",
        "print(\"Loss: {}\".format(eval_loss))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n",
            "Epoch 1/20\n",
            "80/80 [==============================] - 32s 364ms/step - loss: 4.4977 - accuracy: 0.0098 - val_loss: 4.3823 - val_accuracy: 0.0141\n",
            "Epoch 2/20\n",
            "80/80 [==============================] - 28s 353ms/step - loss: 4.3865 - accuracy: 0.0113 - val_loss: 4.3854 - val_accuracy: 0.0156\n",
            "Epoch 3/20\n",
            "80/80 [==============================] - 28s 354ms/step - loss: 4.3837 - accuracy: 0.0113 - val_loss: 4.3770 - val_accuracy: 0.0109\n",
            "Epoch 4/20\n",
            "80/80 [==============================] - 28s 355ms/step - loss: 4.3778 - accuracy: 0.0152 - val_loss: 4.3766 - val_accuracy: 0.0203\n",
            "Epoch 5/20\n",
            "80/80 [==============================] - 28s 352ms/step - loss: 4.3611 - accuracy: 0.0152 - val_loss: 4.4004 - val_accuracy: 0.0172\n",
            "Epoch 6/20\n",
            "80/80 [==============================] - 28s 353ms/step - loss: 4.3603 - accuracy: 0.0141 - val_loss: 4.3721 - val_accuracy: 0.0156\n",
            "Epoch 7/20\n",
            "80/80 [==============================] - 28s 352ms/step - loss: 4.3521 - accuracy: 0.0172 - val_loss: 4.3577 - val_accuracy: 0.0203\n",
            "Epoch 8/20\n",
            "80/80 [==============================] - 28s 356ms/step - loss: 4.3339 - accuracy: 0.0230 - val_loss: 4.3437 - val_accuracy: 0.0234\n",
            "Epoch 9/20\n",
            "68/80 [========================>.....] - ETA: 3s - loss: 4.3220 - accuracy: 0.0244"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-600e5149e0ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     validation_steps=int(math.ceil(validation_generator.n/validation_generator.batch_size)))\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0meval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_accuracy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nLcFpjCYIIK"
      },
      "source": [
        "### **Prediction Results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNYLS_CBYKuY"
      },
      "source": [
        "results = model3.evaluate(test_generator, batch_size=32)\n",
        "print(\"test loss, test acc:\", results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJP6lkQ5OKx9"
      },
      "source": [
        "## **InceptionV3**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Do12nLJVOOJZ",
        "outputId": "d11afc56-2708-430b-a6d7-07fd9a3b04fe"
      },
      "source": [
        "input_shape = (75, 75, 3)\n",
        "learning_rate = 0.001\n",
        "fine_tune = 1\n",
        "dropout = 0.2\n",
        "layer1 = 4096\n",
        "layer2 = 1072\n",
        "\n",
        "#Build data generators\n",
        "from tensorflow.keras.applications import inception_v3\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        preprocessing_function = inception_v3.preprocess_input,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(\n",
        "    preprocessing_function = inception_v3.preprocess_input,\n",
        "    rescale=1./255)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        trainDir,\n",
        "        target_size=input_shape[:2],\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validationDir,\n",
        "        target_size=input_shape[:2]),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        testDir,\n",
        "        target_size=input_shape[:2],\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2560 images belonging to 80 classes.\n",
            "Found 640 images belonging to 80 classes.\n",
            "Found 800 images belonging to 80 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSWCI7TeOeZD"
      },
      "source": [
        "### **Transfer Learning Method #2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThseEBT2OhXH",
        "outputId": "986f9a61-98eb-44af-85d1-dcfd21058365"
      },
      "source": [
        "IV3Model = tf.keras.applications.InceptionV3(input_shape=input_shape,include_top=False,weights='imagenet')\n",
        "\n",
        "if fine_tune > 0:\n",
        "  for layer in IV3Model.layers[:-fine_tune]: layer.trainable = False\n",
        "else:\n",
        "  for layer in IV3Model.layers: layer.trainable = False\n",
        "\n",
        "inputs = tf.keras.Input(shape=input_shape)\n",
        "x = IV3Model(inputs, training=False)\n",
        "'''x = tf.keras.layers.GlobalAveragePooling2D()(x)'''\n",
        "x = tf.keras.layers.Flatten(name=\"flatten\")(x)\n",
        "x = tf.keras.layers.Dense(layer1, activation='relu')(x)\n",
        "x = tf.keras.layers.Dense(layer2, activation='relu')(x)\n",
        "x = tf.keras.layers.Dropout(dropout)(x)\n",
        "outputs = tf.keras.layers.Dense(80, activation='softmax')(x)\n",
        "\n",
        "model4 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model4.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model4.fit(\n",
        "    train_generator,\n",
        "    epochs=30,\n",
        "    validation_data=validation_generator,\n",
        "    steps_per_epoch=int(math.ceil(train_generator.n/train_generator.batch_size)),\n",
        "    validation_steps=int(math.ceil(validation_generator.n/validation_generator.batch_size)))\n",
        "\n",
        "(eval_loss, eval_accuracy) = model4.evaluate(validation_generator, verbose=1)\n",
        "print(\"Accuracy: {:.2f}%\".format(eval_accuracy * 100)) \n",
        "print(\"Loss: {}\".format(eval_loss))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n",
            "Epoch 1/30\n",
            "80/80 [==============================] - 66s 771ms/step - loss: 4.6490 - accuracy: 0.0133 - val_loss: 4.5731 - val_accuracy: 0.0141\n",
            "Epoch 2/30\n",
            "80/80 [==============================] - 64s 797ms/step - loss: 4.4894 - accuracy: 0.0188 - val_loss: 4.4576 - val_accuracy: 0.0266\n",
            "Epoch 3/30\n",
            "80/80 [==============================] - 60s 751ms/step - loss: 4.4007 - accuracy: 0.0270 - val_loss: 4.3458 - val_accuracy: 0.0344\n",
            "Epoch 4/30\n",
            "80/80 [==============================] - 61s 767ms/step - loss: 4.3144 - accuracy: 0.0316 - val_loss: 4.3199 - val_accuracy: 0.0453\n",
            "Epoch 5/30\n",
            "80/80 [==============================] - 62s 769ms/step - loss: 4.2712 - accuracy: 0.0383 - val_loss: 4.3209 - val_accuracy: 0.0266\n",
            "Epoch 6/30\n",
            "80/80 [==============================] - 63s 791ms/step - loss: 4.2157 - accuracy: 0.0492 - val_loss: 4.3222 - val_accuracy: 0.0437\n",
            "Epoch 7/30\n",
            "80/80 [==============================] - 62s 776ms/step - loss: 4.1874 - accuracy: 0.0547 - val_loss: 4.3286 - val_accuracy: 0.0234\n",
            "Epoch 8/30\n",
            "80/80 [==============================] - 62s 776ms/step - loss: 4.1440 - accuracy: 0.0504 - val_loss: 4.3244 - val_accuracy: 0.0344\n",
            "Epoch 9/30\n",
            "80/80 [==============================] - 62s 776ms/step - loss: 4.1214 - accuracy: 0.0539 - val_loss: 4.3168 - val_accuracy: 0.0344\n",
            "Epoch 10/30\n",
            "80/80 [==============================] - 61s 754ms/step - loss: 4.1249 - accuracy: 0.0609 - val_loss: 4.2639 - val_accuracy: 0.0437\n",
            "Epoch 11/30\n",
            "80/80 [==============================] - 63s 789ms/step - loss: 4.0835 - accuracy: 0.0562 - val_loss: 4.2397 - val_accuracy: 0.0312\n",
            "Epoch 12/30\n",
            "80/80 [==============================] - 66s 830ms/step - loss: 4.0201 - accuracy: 0.0754 - val_loss: 4.2717 - val_accuracy: 0.0437\n",
            "Epoch 13/30\n",
            "80/80 [==============================] - 67s 833ms/step - loss: 4.0293 - accuracy: 0.0715 - val_loss: 4.2466 - val_accuracy: 0.0500\n",
            "Epoch 14/30\n",
            "80/80 [==============================] - 61s 766ms/step - loss: 4.0282 - accuracy: 0.0707 - val_loss: 4.3424 - val_accuracy: 0.0484\n",
            "Epoch 15/30\n",
            "80/80 [==============================] - 62s 773ms/step - loss: 4.0094 - accuracy: 0.0773 - val_loss: 4.2874 - val_accuracy: 0.0516\n",
            "Epoch 16/30\n",
            "80/80 [==============================] - 62s 777ms/step - loss: 3.9850 - accuracy: 0.0824 - val_loss: 4.1739 - val_accuracy: 0.0594\n",
            "Epoch 17/30\n",
            "80/80 [==============================] - 62s 772ms/step - loss: 3.9574 - accuracy: 0.0863 - val_loss: 4.2313 - val_accuracy: 0.0453\n",
            "Epoch 18/30\n",
            "80/80 [==============================] - 61s 753ms/step - loss: 3.9279 - accuracy: 0.0898 - val_loss: 4.2294 - val_accuracy: 0.0391\n",
            "Epoch 19/30\n",
            "80/80 [==============================] - 60s 754ms/step - loss: 3.9439 - accuracy: 0.0809 - val_loss: 4.1955 - val_accuracy: 0.0656\n",
            "Epoch 20/30\n",
            "80/80 [==============================] - 61s 757ms/step - loss: 3.9103 - accuracy: 0.0840 - val_loss: 4.2252 - val_accuracy: 0.0484\n",
            "Epoch 21/30\n",
            "80/80 [==============================] - 61s 760ms/step - loss: 3.9139 - accuracy: 0.0801 - val_loss: 4.2644 - val_accuracy: 0.0562\n",
            "Epoch 22/30\n",
            "80/80 [==============================] - 61s 756ms/step - loss: 3.9168 - accuracy: 0.0742 - val_loss: 4.1822 - val_accuracy: 0.0578\n",
            "Epoch 23/30\n",
            "80/80 [==============================] - 61s 759ms/step - loss: 3.8625 - accuracy: 0.0887 - val_loss: 4.2501 - val_accuracy: 0.0594\n",
            "Epoch 24/30\n",
            "80/80 [==============================] - 61s 759ms/step - loss: 3.8703 - accuracy: 0.0961 - val_loss: 4.2482 - val_accuracy: 0.0516\n",
            "Epoch 25/30\n",
            "80/80 [==============================] - 61s 762ms/step - loss: 3.8485 - accuracy: 0.0859 - val_loss: 4.2982 - val_accuracy: 0.0578\n",
            "Epoch 26/30\n",
            "80/80 [==============================] - 61s 768ms/step - loss: 3.8472 - accuracy: 0.1035 - val_loss: 4.2744 - val_accuracy: 0.0437\n",
            "Epoch 27/30\n",
            "80/80 [==============================] - 61s 763ms/step - loss: 3.8352 - accuracy: 0.0965 - val_loss: 4.1997 - val_accuracy: 0.0734\n",
            "Epoch 28/30\n",
            "80/80 [==============================] - 60s 754ms/step - loss: 3.7933 - accuracy: 0.1031 - val_loss: 4.2216 - val_accuracy: 0.0469\n",
            "Epoch 29/30\n",
            "80/80 [==============================] - 61s 759ms/step - loss: 3.8075 - accuracy: 0.1078 - val_loss: 4.2487 - val_accuracy: 0.0422\n",
            "Epoch 30/30\n",
            "80/80 [==============================] - 61s 766ms/step - loss: 3.7929 - accuracy: 0.1059 - val_loss: 4.2167 - val_accuracy: 0.0547\n",
            "20/20 [==============================] - 11s 535ms/step - loss: 4.2167 - accuracy: 0.0547\n",
            "Accuracy: 5.47%\n",
            "Loss: 4.216727256774902\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fvD-nOcYNV1"
      },
      "source": [
        "### **Prediction Results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uf7rg5MjYPph",
        "outputId": "d97d88f0-cf0e-4276-d297-bc32079171b6"
      },
      "source": [
        "results = model4.evaluate(test_generator, batch_size=32)\n",
        "print(\"test loss, test acc:\", results)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25/25 [==============================] - 14s 555ms/step - loss: 4.2989 - accuracy: 0.0800\n",
            "test loss, test acc: [4.298933029174805, 0.07999999821186066]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}